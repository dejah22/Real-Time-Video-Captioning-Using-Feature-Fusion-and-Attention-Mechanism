{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFzDut_SPV4F",
        "outputId": "1610649d-908d-49b7-901c-fc86ba4435d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "mount success\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print('mount success')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5sK-Bp8PYPz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHIPpk809FZQ"
      },
      "outputs": [],
      "source": [
        "# test path\n",
        "test_path = '/content/drive/Shareddrives/Final-Year-Project/github-data/testing_data/split-video'\n",
        "\n",
        "# test features\n",
        "test_features_path = '/content/drive/Shareddrives/msvd-test-feats/longer-test/custom_feat'\n",
        "\n",
        "# store captions path\n",
        "captions_path = os.path.join(test_path, 'captions')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "video_folder = 'baby_laughing'\n",
        "video_path = os.path.join(test_path, video_folder)"
      ],
      "metadata": {
        "id": "kCnTMcdFLSsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras_preprocessing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvqlltMeecpU",
        "outputId": "626d7836-c372-49b2-d5fa-1f1a587162b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras_preprocessing\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from keras_preprocessing) (1.25.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from keras_preprocessing) (1.16.0)\n",
            "Installing collected packages: keras_preprocessing\n",
            "Successfully installed keras_preprocessing-1.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MODEL FOR INFERENCE"
      ],
      "metadata": {
        "id": "iFvX52UB3vo8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "from keras.models import Model, load_model\n",
        "import joblib"
      ],
      "metadata": {
        "id": "uDN4jkjGxMKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_model_path = '/content/drive/Shareddrives/FYP-models/without_cust_vocab_3700'\n",
        "encoder_name = 'encoder_model.h5'\n",
        "decoder_name = 'decoder_model_weights.h5'\n",
        "num_encoder_tokens = 4126\n",
        "num_decoder_tokens = 3700\n",
        "latent_dim = 512\n",
        "time_steps_encoder = 80\n",
        "\n",
        "model_name = save_model_path.split('/')[-1]\n",
        "print(model_name)"
      ],
      "metadata": {
        "id": "t9ZSYdFsxO3R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1d9b410-e0ee-43e4-bc66-8b24044cb848"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "without_cust_vocab_3700\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def inference_model():\n",
        "    \"\"\"Returns the model that will be used for inference\"\"\"\n",
        "    with open(os.path.join(save_model_path, 'tokenizer' + str(num_decoder_tokens)), 'rb') as file:\n",
        "        tokenizer = joblib.load(file)\n",
        "    # loading encoder model. This remains the same\n",
        "    inf_encoder_model = load_model(os.path.join(save_model_path, encoder_name))\n",
        "\n",
        "    # Manually compile the encoder model\n",
        "    inf_encoder_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "    # inference decoder model loading\n",
        "    decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
        "    decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "    decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "    decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "    decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "    decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
        "    decoder_states = [state_h, state_c]\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "    inf_decoder_model = Model(\n",
        "        [decoder_inputs] + decoder_states_inputs,\n",
        "        [decoder_outputs] + decoder_states)\n",
        "    inf_decoder_model.load_weights(os.path.join(save_model_path, decoder_name))\n",
        "    return tokenizer, inf_encoder_model, inf_decoder_model\n"
      ],
      "metadata": {
        "id": "4flXqObi3vPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TESTING\n"
      ],
      "metadata": {
        "id": "wNb_Fj35vM5y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### IMPORTS"
      ],
      "metadata": {
        "id": "KCRqmwqsvfeK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dill\n",
        "!pip show h5py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37XKkiT9vTtM",
        "outputId": "3efa97b9-7ff9-4391-9146-b48cd3e2114b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dill\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/116.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m112.6/116.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dill\n",
            "Successfully installed dill-0.3.8\n",
            "Name: h5py\n",
            "Version: 3.9.0\n",
            "Summary: Read and write HDF5 files from Python\n",
            "Home-page: \n",
            "Author: \n",
            "Author-email: Andrew Collette <andrew.collette@gmail.com>\n",
            "License: BSD-3-Clause\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: numpy\n",
            "Required-by: h5netcdf, tensorflow\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import functools\n",
        "import operator\n",
        "import os\n",
        "import time\n",
        "import h5py\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "EszZAWmvvXuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TEST VIDEO CAPTIONING"
      ],
      "metadata": {
        "id": "en6d2pqCwzZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_list_to_text_file(lst, filename):\n",
        "    with open(filename, 'w') as file:\n",
        "        for item in lst:\n",
        "            file.write(str(item) + '\\n')"
      ],
      "metadata": {
        "id": "OzabJlwfNGxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VideoDescriptionInference(object):\n",
        "    \"\"\"\n",
        "        Initialize the parameters for the model\n",
        "        \"\"\"\n",
        "    def __init__(self):\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_encoder_tokens = num_encoder_tokens\n",
        "        self.num_decoder_tokens = num_decoder_tokens\n",
        "        self.time_steps_encoder = time_steps_encoder\n",
        "        self.max_probability = -1\n",
        "\n",
        "        # models\n",
        "        self.tokenizer, self.inf_encoder_model,  self.inf_decoder_model = inference_model()\n",
        "        self.save_model_path = save_model_path\n",
        "        self.test_path = test_path\n",
        "        self.test_features_path = test_features_path\n",
        "        self.search_type = 'greedy'\n",
        "\n",
        "    def greedy_search(self, loaded_array):\n",
        "        \"\"\"\n",
        "\n",
        "        :param f: the loaded numpy array after creating videos to frames and extracting features\n",
        "        :return: the final sentence which has been predicted greedily\n",
        "        \"\"\"\n",
        "        inv_map = self.index_to_word()\n",
        "        states_value = self.inf_encoder_model.predict(loaded_array.reshape(-1, self.time_steps_encoder, self.num_encoder_tokens))\n",
        "        target_seq = np.zeros((1, 1, self.num_decoder_tokens))\n",
        "        final_sentence = ''\n",
        "        target_seq[0, 0, self.tokenizer.word_index['bos']] = 1\n",
        "        for i in range(15):\n",
        "            output_tokens, h, c = self.inf_decoder_model.predict([target_seq] + states_value)\n",
        "            states_value = [h, c]\n",
        "            output_tokens = output_tokens.reshape(self.num_decoder_tokens)\n",
        "            y_hat = np.argmax(output_tokens)\n",
        "            if y_hat == 0:\n",
        "                continue\n",
        "            if inv_map[y_hat] is None:\n",
        "                break\n",
        "            if inv_map[y_hat] == 'eos':\n",
        "                break\n",
        "            else:\n",
        "                final_sentence = final_sentence + inv_map[y_hat] + ' '\n",
        "                target_seq = np.zeros((1, 1, self.num_decoder_tokens))\n",
        "                target_seq[0, 0, y_hat] = 1\n",
        "        return final_sentence\n",
        "\n",
        "    def decode_sequence2bs(self, input_seq):\n",
        "        states_value = self.inf_encoder_model.predict(input_seq)\n",
        "        target_seq = np.zeros((1, 1, self.num_decoder_tokens))\n",
        "        target_seq[0, 0, self.tokenizer.word_index['bos']] = 1\n",
        "        self.beam_search(target_seq, states_value, [], [], 0)\n",
        "        return decode_seq\n",
        "\n",
        "    def beam_search(self, target_seq, states_value, prob, path, lens):\n",
        "        \"\"\"\n",
        "\n",
        "        :param target_seq: the array that is fed into the model to predict the next word\n",
        "        :param states_value: previous state that is fed into the lstm cell\n",
        "        :param prob: probability of predicting a word\n",
        "        :param path: list of words from each sentence\n",
        "        :param lens: number of words\n",
        "        :return: final sentence\n",
        "        \"\"\"\n",
        "        global decode_seq\n",
        "        node = 2\n",
        "        output_tokens, h, c = self.inf_decoder_model.predict(\n",
        "            [target_seq] + states_value)\n",
        "        output_tokens = output_tokens.reshape(self.num_decoder_tokens)\n",
        "        sampled_token_index = output_tokens.argsort()[-node:][::-1]\n",
        "        states_value = [h, c]\n",
        "        for i in range(node):\n",
        "            if sampled_token_index[i] == 0:\n",
        "                sampled_char = ''\n",
        "            else:\n",
        "                sampled_char = list(self.tokenizer.word_index.keys())[\n",
        "                    list(self.tokenizer.word_index.values()).index(sampled_token_index[i])]\n",
        "            MAX_LEN = 12\n",
        "            if sampled_char != 'eos' and lens <= MAX_LEN:\n",
        "                p = output_tokens[sampled_token_index[i]]\n",
        "                if sampled_char == '':\n",
        "                    p = 1\n",
        "                prob_new = list(prob)\n",
        "                prob_new.append(p)\n",
        "                path_new = list(path)\n",
        "                path_new.append(sampled_char)\n",
        "                target_seq = np.zeros((1, 1, self.num_decoder_tokens))\n",
        "                target_seq[0, 0, sampled_token_index[i]] = 1.\n",
        "                self.beam_search(target_seq, states_value, prob_new, path_new, lens + 1)\n",
        "            else:\n",
        "                p = output_tokens[sampled_token_index[i]]\n",
        "                prob_new = list(prob)\n",
        "                prob_new.append(p)\n",
        "                p = functools.reduce(operator.mul, prob_new, 1)\n",
        "                if p > self.max_probability:\n",
        "                    decode_seq = path\n",
        "                    self.max_probability = p\n",
        "\n",
        "    def decoded_sentence_tuning(self, decoded_sentence):\n",
        "        # tuning sentence\n",
        "        decode_str = []\n",
        "        filter_string = ['bos', 'eos']\n",
        "        uni_gram = {}\n",
        "        last_string = \"\"\n",
        "        for idx2, c in enumerate(decoded_sentence):\n",
        "            if c in uni_gram:\n",
        "                uni_gram[c] += 1\n",
        "            else:\n",
        "                uni_gram[c] = 1\n",
        "            if last_string == c and idx2 > 0:\n",
        "                continue\n",
        "            if c in filter_string:\n",
        "                continue\n",
        "            if len(c) > 0:\n",
        "                decode_str.append(c)\n",
        "            if idx2 > 0:\n",
        "                last_string = c\n",
        "        return decode_str\n",
        "\n",
        "    def index_to_word(self):\n",
        "        # inverts word tokenizer\n",
        "        index_to_word = {value: key for key, value in self.tokenizer.word_index.items()}\n",
        "        return index_to_word\n",
        "\n",
        "    def get_test_data(self):\n",
        "        filename_test = []\n",
        "        feat_test = []\n",
        "\n",
        "        check_num = 0\n",
        "        video_list = os.listdir(video_path)\n",
        "        for video in video_list:\n",
        "            # getting filename for the test video\n",
        "            file_name = video.split(\".\")[0]\n",
        "            filename_test.append(file_name)\n",
        "\n",
        "            '''\n",
        "            # getting cnn features for the test video\n",
        "            cnn_path = os.path.join(self.test_features_path, 'feat', file_name + '.npy')\n",
        "            if os.path.exists(cnn_path): #features exist\n",
        "                cnn_f = np.load(cnn_path)\n",
        "            else:\n",
        "                print(\"features not found\")\n",
        "            feat_test.append(cnn_f)\n",
        "            '''\n",
        "\n",
        "            # getting cnn and yolo features for the test video\n",
        "            cnn_path = os.path.join(self.test_features_path, 'feat', file_name + '.npy')\n",
        "            yolo_path = os.path.join(self.test_features_path, 'yolo-feat', file_name + '.npy')\n",
        "            if os.path.exists(cnn_path) and os.path.exists(yolo_path): #features exist\n",
        "                cnn_f = np.load(cnn_path)\n",
        "                yolo_f = np.load(yolo_path)\n",
        "            else:\n",
        "                print(\"features not found\")\n",
        "            yolo_f = yolo_f.reshape(-1, 30)\n",
        "            #concatenate arrays\n",
        "            combined_f = np.concatenate((cnn_f, yolo_f), axis=1)\n",
        "\n",
        "            feat_test.append(combined_f)\n",
        "\n",
        "            check_num += 1\n",
        "            print(f\"{check_num} feature files loaded\")\n",
        "\n",
        "\n",
        "        feat_test = np.array(feat_test)\n",
        "\n",
        "        return feat_test, filename_test\n",
        "\n",
        "    def test(self):\n",
        "        feat_test, filename_test = self.get_test_data()\n",
        "\n",
        "        # generate inference test outputs\n",
        "        with open(os.path.join(captions_path, f'test_video_captions_generated_{model_name}_{video_folder}.txt'), 'w') as file:\n",
        "            for idx, feat in enumerate(feat_test):\n",
        "                print(idx)\n",
        "                file.write(filename_test[idx] + ',')\n",
        "                if self.search_type == 'greedy':\n",
        "                    start = time.time()\n",
        "                    decoded_sentence = self.greedy_search(feat.reshape(-1, self.time_steps_encoder, self.num_encoder_tokens))\n",
        "                    file.write(decoded_sentence + ',{:.2f}'.format(time.time()-start))\n",
        "                else:\n",
        "                    start = time.time()\n",
        "                    decoded_sentence = self.decode_sequence2bs(feat.reshape(-1, self.time_steps_encoder, self.num_encoder_tokens))\n",
        "                    decode_str = self.decoded_sentence_tuning(decoded_sentence)\n",
        "                    for d in decode_str:\n",
        "                        file.write(d + ' ')\n",
        "                    file.write(',{:.2f}'.format(time.time() - start))\n",
        "                file.write('\\n')\n",
        "\n",
        "                # re-init max prob\n",
        "                self.max_probability = -1"
      ],
      "metadata": {
        "id": "X-IbBxmBu5Is"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    video_to_text = VideoDescriptionInference()\n",
        "    video_to_text.test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvIDPBb3xjTu",
        "outputId": "a4180837-ec5d-461a-b41c-88a4c6c04539"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 feature files loaded\n",
            "2 feature files loaded\n",
            "3 feature files loaded\n",
            "4 feature files loaded\n",
            "5 feature files loaded\n",
            "6 feature files loaded\n",
            "0\n",
            "1/1 [==============================] - 0s 469ms/step\n",
            "1/1 [==============================] - 0s 456ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "2\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "3\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "4\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "5\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
